1.1 데이터 파이프라인
 초기 단계 스타트업에서 기획자로부터 첫 번째 데이터 요건이 들어온다.
  > 데이터를 보고 싶다. 실시간이면 좋겠다.
  > 개발자들은 기능 만드는데 바쁘니 쿼리보다는 UI를 통해 이런저런 데이터를 보거나 가공할 수 있어야 한다.

가장 쉬운 방법은 실제 서비스 DB에 붙어 데이터를 보는 것. 물론 당연히 그러면 안됨.
따라서 AWS RDS에 Read Replica를 이용하면 아주 약간의 지연이 있을 뿐, 실제 서비스 DB를 실시간 / 비동기로 복제해 데이터를 확인할 수 있다.

*데이터 엔지니어가 도입하는 모든 데이터 도구는, 데이터로 일하는 회사 구성원 전부의 생산성에 영향을 미친다.
따라서 단순히 '이 기능이 좋아'보다는 다음 내용들을 고려하여 제공해야 한다.
 > 여러 시스템과의 연동
 > 코드 수정 및 확장 가능성
 > 다양한 활용처 : 대용량 CSV데이터 다운로드/데이터 탐색/대시보드 등등


> 다양한 스토리지(RDB, NoSQL)를 지원하고, 서로 다른 스토리지의 데이터를 Join 할 수 있어야함
> 스토리지에 상관 없이 일관되고 사용성이 편리한 SQL 문법을 제공해야 함
> 대규모 데이터를 메모리에서 빠르게 처리할 수 있어야 함

RDB의 데이터는 무한히 적재할 수 없다. 주문, 예약 등 비즈니스 크리티컬한 데이터가 아니라 단순 History성 테이블의 경우에는 어느 순간에는 특정 시점을 기준으로 과거 데이터는 서비스 테이블에서 분리해 보존처리 해야할 수 있다.
 > '주기적으로' 데이터를 대량의 컴퓨팅을 수행할 수 있는 저장소로 옮겨오는 작업이 필요하게 된다.
  RDB, NoSQL(Redis, ElasticSerach)에서 데이터를 읽어 AWS S3 / HDFS 등에 저장한다.


AWS DMS(Database Migration Service)
 > DMS는 복제 컴퓨팅을 수행할 리소스를 사용자가 지정할 수 있다.
 > 컬럼 및 컬럼 값에 대해 필터링 기능과 변경 기능(일부)을 제공한다.
 > CDC(Change Data Capture) 등 RDB 입수 관련된 다양한 기능을 제공한다.


* 데이터 수집 과정에서 기술을 선정할 떄 고려해야 할 것은 크게 3가지
 1. 다양한 데이터 저장소와 포맷을 입수할 수 있는가?
  - RDB, NoSQL 부터 시작해 사내 사용자가 사용하는 Google SHeet, CSV, JSON 등 단순 파일 포맷은 물론 Client/Server가 보내는 Event 등 다양한 데이터를 어떤 저장소에서 / 어떤 프레임워크가 읽어 / 어느 곳에 저장할지

 2. 데이터 사이즈가 커짐에 따라 수집 인프라를 확장할 수 있는가?
  - 데이터를 읽거나, 내보내는 저장소 'Stroge'라는 이름에서 볼 수 있듯이 일반적으로 확장이 가능함
  - 데이터를 수집하기 위해 컴퓨팅을수행하는 중간 프로세싱 단계의 인프라가 확장이 되지 않는 경우가 있다.(예. Airflow에서 단일 Worker를 사용해 데이터를 처리)
  - 따라서 Spark 등의 분산처리 / 메모리 + Disk를 지원하는 데이터 처리 프레임워크를 선택해 추후 데이터 사이즈가 커질 때 컴퓨팅 리소스를 늘려, "원하는 시점에, 원하는 속도로" 결과물을 낼수 있어야 한다.

 3. 인프라를 운영하는 엔지니어의 운영 부담
  - 다양한 도구를 활용하는 건 엔지니어 개인으로서는 재밌는 도전일 수 있으나, 동일한 종류의 데이터 입수에 Airflow, Spark, Nifi 등 다양한 프레임워크를 활용하면, 이것을 운영해야 하는 엔지니어 입장에서는 규모가 커졌을 때 문제가 된다.
  - 예를 들어, 입수해야 할테이블이 수십개라면 원하는 다양한도구를 사용해도 문제가 없지만 개별 엔지니어가 관리하는테이블이 수백개, 수천개에 이른다면 개인의 컨텍스트 스위칭으로 인한 인지적 부하는 물론 휴가/백업/채용등조직 관리적인 이슈까지 포함하면 프레임워크를 단일화 하는 편이 낫다.


로그(Client / Server) 데이터 입수

 일반적으로 DB등 스토리지에서 입수하는 데이터가 아니라 사용자의 Client(App / Web) 등에서 발생하는 데이터를 '이벤트' 또는 '로그'라고 부른다.
  > API 등에서 Logging Framework 를 통해 만다는 일반 Application '로그'일 수도 있고(INFO, ERROR 등)
  > 비즈니스 용도로 Protobuf, Avro 등 포맷을 갖추어 전송하는 '이벤트'일 수도 있다.

   - 사용자의 특정 페이지 방문등 '이벤트'데이터는 '변경'되지 않는다.
   - 분석 용도가 아니라면 당장은 서비스에 쓸, 다시 말해 API에서 DB접근해서 다시 내보내야 할 경우가 거의 없다.
   - RDB에 쌓는 일반적인 트랜잭션 데이터에 비해, 사용자 활동 이력은 '매우'많다. 주분 1건에 대해 어떤 활동을 하는지가 훨 씬 많다.
   - RDB는 API에서 '접근하기 쉬운' 저장소이지, 비용 효율적인 저장소는 아니다.

  
후반부

 서비스가 좀 더 커지면 데이터 팀에서는 조회를 위한 수집 및 가공이 아니라 서비스에도 관여하게 된다.
 추천 / 통계 / 가격 모델링 등을 비롯해 머신러닝을 위한 인프라(MLOps)를 구축할 수도 있다.

 일반적인 요구사항들
  - 실시간 데이터 '집계'를 필요로 함(과금을 위한 광고 통계 / 인기도 등)
  - 실시간 데이터 가공 및 저장을 필요로 함(사용자 및 상품 Profile 등을 만들거나 이를 바탕으로 특정 이벤트 발생시 사용자에게 Push를 전송 등)
  - 마케팅 플랫폼을 구축하며 외부 광고(SNS 등) 시스템과 사용자의 광고 ID / 과거 이벤트 이력기반으로 연동
  - A/B 테스팅 등을 위한 실험 플랫폼의 필요성이 대두
  - 배치 및 실시간 집계를 하나의 테이블로 '묶어서' 보고자 하는 사용자 발생
  - 과거 수년치의 데이터를 집계하는 경우가 많아짐
  - Cardinality 가 높은 데이터 가공이 많아짐(사용자 ID 등이 사용되는 Feature 집계 등)
  - 데이터가 단순한 메트릭이 아니라 Vector 화 된 형태로 서빙되거나 시스템에서 조회 될 수 있음(Feature 테이블 등)

  [링크](https://speakerdeck.com/1ambda/machine-learning-on-kubernetes?slide=8)
  
  change
  